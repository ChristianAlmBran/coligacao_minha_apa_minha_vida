# -*- coding: utf-8 -*-
"""gov_plans_treaty_2020_2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcHRPvf4_-TwXhe0QBt1QN1q4gg2PODE
"""

!pip install transformers torch scikit-learn numpy pandas

import os
import pandas as pd

# 2020-2024
plano_path = '/content/drive/MyDrive/planos_gov_cand_mun/2020_2024'
tratado_path = '/content/drive/MyDrive/planos_gov_cand_mun'

# Reading the files and creating a single df
def read_text_files(folder_path, doc_type):
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            data.append({'doc_id': filename, 'doc_type': doc_type, 'text': text_content})
    return pd.DataFrame(data)

planos_df = read_text_files(plano_path, 'plano_governo')
tratado_df = read_text_files(tratado_path, 'tratado_internacional')

all_documents_df = pd.concat([planos_df, tratado_df], ignore_index=True)
print(all_documents_df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained Sentence Transformer model
# 'paraphrase-multilingual-MiniLM-L12-v2' is a good general-purpose multilingual model
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings using the Sentence Transformer model
all_documents_df['sentence_embedding'] = all_documents_df['text'].apply(
    lambda x: sentence_model.encode(x)
)

# Separate reference vector (tratado) and plans embeddings using the new embeddings
ue_reference_sentence_embedding = all_documents_df[all_documents_df['doc_type'] == 'tratado_internacional']['sentence_embedding'].values[0]

# Calculate cosine similarity using the new embeddings
planos_sentence_embeddings = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['sentence_embedding'].tolist()
planos_doc_ids = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['doc_id'].tolist()

sentence_similarities = [cosine_similarity(e.reshape(1, -1), ue_reference_sentence_embedding.reshape(1, -1))[0][0] for e in planos_sentence_embeddings]

# Create a new DataFrame for results with the new similarities
results_sentence_df = pd.DataFrame({
    'doc_id': planos_doc_ids,
    'similaridade_ue_sentence_transformer': sentence_similarities
})

print(results_sentence_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(results_sentence_df['similaridade_ue_sentence_transformer'], bins=20, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Sentence Transformer)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Sort the results by similarity score in descending order to get the most similar
most_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=False)

# Sort the results by similarity score in ascending order to get the least similar
least_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=True)

print("Most Similar Plans to the Treaty:")
display(most_similar_plans.head())

print("\nLeast Similar Plans to the Treaty:")
display(least_similar_plans.head())

# Calculate the average cosine similarity
average_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].mean()
print(f"Average Cosine Similarity - 2020: {average_similarity}")

# Calculate the median cosine similarity
median_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].median()
print(f"Median Cosine Similarity - 2020: {median_similarity}")

"""**2020**-----------------------------------------------"""

import os
import pandas as pd

# 2020
plano_path = '/content/drive/MyDrive/planos_gov_cand_mun/2020_txt'
tratado_path = '/content/drive/MyDrive/planos_gov_cand_mun'

# Reading the files and creating a single df
def read_text_files(folder_path, doc_type):
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            data.append({'doc_id': filename, 'doc_type': doc_type, 'text': text_content})
    return pd.DataFrame(data)

planos_df = read_text_files(plano_path, 'plano_governo')
tratado_df = read_text_files(tratado_path, 'tratado_internacional')

all_documents_df = pd.concat([planos_df, tratado_df], ignore_index=True)
print(all_documents_df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained Sentence Transformer model
# 'paraphrase-multilingual-MiniLM-L12-v2' is a good general-purpose multilingual model
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings using the Sentence Transformer model
all_documents_df['sentence_embedding'] = all_documents_df['text'].apply(
    lambda x: sentence_model.encode(x)
)

# Separate reference vector (tratado) and plans embeddings using the new embeddings
ue_reference_sentence_embedding = all_documents_df[all_documents_df['doc_type'] == 'tratado_internacional']['sentence_embedding'].values[0]

# Calculate cosine similarity using the new embeddings
planos_sentence_embeddings = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['sentence_embedding'].tolist()
planos_doc_ids = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['doc_id'].tolist()

sentence_similarities = [cosine_similarity(e.reshape(1, -1), ue_reference_sentence_embedding.reshape(1, -1))[0][0] for e in planos_sentence_embeddings]

# Create a new DataFrame for results with the new similarities
results_sentence_df = pd.DataFrame({
    'doc_id': planos_doc_ids,
    'similaridade_ue_sentence_transformer': sentence_similarities
})

print(results_sentence_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(results_sentence_df['similaridade_ue_sentence_transformer'], bins=20, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Sentence Transformer) 2020')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Sort the results by similarity score in descending order to get the most similar
most_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=False)

# Sort the results by similarity score in ascending order to get the least similar
least_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=True)

print("Most Similar Plans to the Treaty:")
display(most_similar_plans.head())

print("\nLeast Similar Plans to the Treaty:")
display(least_similar_plans.head())

# Calculate the average cosine similarity
average_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].mean()
print(f"Average Cosine Similarity - 2020: {average_similarity}")

# Calculate the median cosine similarity
median_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].median()
print(f"Median Cosine Similarity - 2020: {median_similarity}")

# Somente 2024
import os
import pandas as pd

# 2024
plano_path = '/content/drive/MyDrive/planos_gov_cand_mun/2024_txt'
tratado_path = '/content/drive/MyDrive/planos_gov_cand_mun'

# Reading the files and creating a single df
def read_text_files(folder_path, doc_type):
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            data.append({'doc_id': filename, 'doc_type': doc_type, 'text': text_content})
    return pd.DataFrame(data)

planos_df = read_text_files(plano_path, 'plano_governo')
tratado_df = read_text_files(tratado_path, 'tratado_internacional')

all_documents_df = pd.concat([planos_df, tratado_df], ignore_index=True)
print(all_documents_df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained Sentence Transformer model
# 'paraphrase-multilingual-MiniLM-L12-v2' is a good general-purpose multilingual model
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings using the Sentence Transformer model
all_documents_df['sentence_embedding'] = all_documents_df['text'].apply(
    lambda x: sentence_model.encode(x)
)

# Separate reference vector (tratado) and plans embeddings using the new embeddings
ue_reference_sentence_embedding = all_documents_df[all_documents_df['doc_type'] == 'tratado_internacional']['sentence_embedding'].values[0]

# Calculate cosine similarity using the new embeddings
planos_sentence_embeddings = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['sentence_embedding'].tolist()
planos_doc_ids = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['doc_id'].tolist()

sentence_similarities = [cosine_similarity(e.reshape(1, -1), ue_reference_sentence_embedding.reshape(1, -1))[0][0] for e in planos_sentence_embeddings]

# Create a new DataFrame for results with the new similarities
results_sentence_df = pd.DataFrame({
    'doc_id': planos_doc_ids,
    'similaridade_ue_sentence_transformer': sentence_similarities
})

print(results_sentence_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(results_sentence_df['similaridade_ue_sentence_transformer'], bins=20, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Sentence Transformer) 2024')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Sort the results by similarity score in descending order to get the most similar
most_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=False)

# Sort the results by similarity score in ascending order to get the least similar
least_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=True)

print("Most Similar Plans to the Treaty:")
display(most_similar_plans.head())

print("\nLeast Similar Plans to the Treaty:")
display(least_similar_plans.head())

# Calculate the average cosine similarity
average_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].mean()
print(f"Average Cosine Similarity - 2024: {average_similarity}")

# Calculate the median cosine similarity
median_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].median()
print(f"Median Cosine Similarity - 2024: {median_similarity}")

"""Tamandar√© ----------------------------------------"""

import os
import pandas as pd

# 2020-2024
plano_path = '/content/drive/MyDrive/planos_gov_cand_mun/tamandare'
tratado_path = '/content/drive/MyDrive/planos_gov_cand_mun'

# Reading the files and creating a single df
def read_text_files(folder_path, doc_type):
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            data.append({'doc_id': filename, 'doc_type': doc_type, 'text': text_content})
    return pd.DataFrame(data)

planos_df = read_text_files(plano_path, 'plano_governo')
tratado_df = read_text_files(tratado_path, 'tratado_internacional')

all_documents_df = pd.concat([planos_df, tratado_df], ignore_index=True)
print(all_documents_df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained Sentence Transformer model
# 'paraphrase-multilingual-MiniLM-L12-v2' is a good general-purpose multilingual model
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings using the Sentence Transformer model
all_documents_df['sentence_embedding'] = all_documents_df['text'].apply(
    lambda x: sentence_model.encode(x)
)

# Separate reference vector (tratado) and plans embeddings using the new embeddings
ue_reference_sentence_embedding = all_documents_df[all_documents_df['doc_type'] == 'tratado_internacional']['sentence_embedding'].values[0]

# Calculate cosine similarity using the new embeddings
planos_sentence_embeddings = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['sentence_embedding'].tolist()
planos_doc_ids = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['doc_id'].tolist()

sentence_similarities = [cosine_similarity(e.reshape(1, -1), ue_reference_sentence_embedding.reshape(1, -1))[0][0] for e in planos_sentence_embeddings]

# Create a new DataFrame for results with the new similarities
results_sentence_df = pd.DataFrame({
    'doc_id': planos_doc_ids,
    'similaridade_ue_sentence_transformer': sentence_similarities
})

print(results_sentence_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(results_sentence_df['similaridade_ue_sentence_transformer'], bins=20, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Sentence Transformer)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Sort the results by similarity score in descending order to get the most similar
most_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=False)

# Sort the results by similarity score in ascending order to get the least similar
least_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=True)

print("Most Similar Plans to the Treaty:")
display(most_similar_plans.head())

print("\nLeast Similar Plans to the Treaty:")
display(least_similar_plans.head())

# Calculate the average cosine similarity
average_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].mean()
print(f"Average Cosine Similarity - 2024: {average_similarity}")

# Calculate the median cosine similarity
median_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].median()
print(f"Median Cosine Similarity - 2024: {median_similarity}")

"""Turistic cities -----------------------------------"""

import os
import pandas as pd

# 2020-2024
plano_path = '/content/drive/MyDrive/planos_gov_cand_mun/municipios_turisticos'
tratado_path = '/content/drive/MyDrive/planos_gov_cand_mun'

# Reading the files and creating a single df
def read_text_files(folder_path, doc_type):
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            data.append({'doc_id': filename, 'doc_type': doc_type, 'text': text_content})
    return pd.DataFrame(data)

planos_df = read_text_files(plano_path, 'plano_governo')
tratado_df = read_text_files(tratado_path, 'tratado_internacional')

all_documents_df = pd.concat([planos_df, tratado_df], ignore_index=True)
print(all_documents_df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained Sentence Transformer model
# 'paraphrase-multilingual-MiniLM-L12-v2' is a good general-purpose multilingual model
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings using the Sentence Transformer model
all_documents_df['sentence_embedding'] = all_documents_df['text'].apply(
    lambda x: sentence_model.encode(x)
)

# Separate reference vector (tratado) and plans embeddings using the new embeddings
ue_reference_sentence_embedding = all_documents_df[all_documents_df['doc_type'] == 'tratado_internacional']['sentence_embedding'].values[0]

# Calculate cosine similarity using the new embeddings
planos_sentence_embeddings = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['sentence_embedding'].tolist()
planos_doc_ids = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['doc_id'].tolist()

sentence_similarities = [cosine_similarity(e.reshape(1, -1), ue_reference_sentence_embedding.reshape(1, -1))[0][0] for e in planos_sentence_embeddings]

# Create a new DataFrame for results with the new similarities
results_sentence_df = pd.DataFrame({
    'doc_id': planos_doc_ids,
    'similaridade_ue_sentence_transformer': sentence_similarities
})

print(results_sentence_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(results_sentence_df['similaridade_ue_sentence_transformer'], bins=20, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Sentence Transformer)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Sort the results by similarity score in descending order to get the most similar
most_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=False)

# Sort the results by similarity score in ascending order to get the least similar
least_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=True)

print("Most Similar Plans to the Treaty:")
display(most_similar_plans.head())

print("\nLeast Similar Plans to the Treaty:")
display(least_similar_plans.head())

# Calculate the average cosine similarity
average_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].mean()
print(f"Average Cosine Similarity - 2024: {average_similarity}")

# Calculate the median cosine similarity
median_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].median()
print(f"Median Cosine Similarity - 2024: {median_similarity}")

"""Prepared cities -------------------------"""

import os
import pandas as pd

# 2020-2024
plano_path = '/content/drive/MyDrive/planos_gov_cand_mun/municipios_mais_preparados'
tratado_path = '/content/drive/MyDrive/planos_gov_cand_mun'

# Reading the files and creating a single df
def read_text_files(folder_path, doc_type):
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                text_content = f.read()
            data.append({'doc_id': filename, 'doc_type': doc_type, 'text': text_content})
    return pd.DataFrame(data)

planos_df = read_text_files(plano_path, 'plano_governo')
tratado_df = read_text_files(tratado_path, 'tratado_internacional')

all_documents_df = pd.concat([planos_df, tratado_df], ignore_index=True)
print(all_documents_df.head())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load a pre-trained Sentence Transformer model
# 'paraphrase-multilingual-MiniLM-L12-v2' is a good general-purpose multilingual model
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Generate embeddings using the Sentence Transformer model
all_documents_df['sentence_embedding'] = all_documents_df['text'].apply(
    lambda x: sentence_model.encode(x)
)

# Separate reference vector (tratado) and plans embeddings using the new embeddings
ue_reference_sentence_embedding = all_documents_df[all_documents_df['doc_type'] == 'tratado_internacional']['sentence_embedding'].values[0]

# Calculate cosine similarity using the new embeddings
planos_sentence_embeddings = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['sentence_embedding'].tolist()
planos_doc_ids = all_documents_df[all_documents_df['doc_type'] == 'plano_governo']['doc_id'].tolist()

sentence_similarities = [cosine_similarity(e.reshape(1, -1), ue_reference_sentence_embedding.reshape(1, -1))[0][0] for e in planos_sentence_embeddings]

# Create a new DataFrame for results with the new similarities
results_sentence_df = pd.DataFrame({
    'doc_id': planos_doc_ids,
    'similaridade_ue_sentence_transformer': sentence_similarities
})

print(results_sentence_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(results_sentence_df['similaridade_ue_sentence_transformer'], bins=20, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Sentence Transformer)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Sort the results by similarity score in descending order to get the most similar
most_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=False)

# Sort the results by similarity score in ascending order to get the least similar
least_similar_plans = results_sentence_df.sort_values(by='similaridade_ue_sentence_transformer', ascending=True)

print("Most Similar Plans to the Treaty:")
display(most_similar_plans.head())

print("\nLeast Similar Plans to the Treaty:")
display(least_similar_plans.head())

# Calculate the average cosine similarity
average_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].mean()
print(f"Average Cosine Similarity - 2024: {average_similarity}")

# Calculate the median cosine similarity
median_similarity = results_sentence_df['similaridade_ue_sentence_transformer'].median()
print(f"Median Cosine Similarity - 2024: {median_similarity}")